{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4937dbf",
   "metadata": {},
   "source": [
    "# TfL cycling usage data 2021 - 2023 Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367baac",
   "metadata": {},
   "source": [
    "## Load all input data from TFL cycling usage data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"tfl_csv_files\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "i = 0\n",
    "print(f\"Looking for CSV files in: {os.path.abspath(folder_path)}\")\n",
    "\n",
    "# --- Define the mapping from variant names to standard names ---\n",
    "column_mapping = {\n",
    "    \"Start date\": \"Start Date\",           # Standardize 'Start Date'\n",
    "    \"End date\": \"End Date\",             # Standardize 'End Date'\n",
    "    \"Start station\": \"StartStation Name\", # Standardize 'Start Station Name'\n",
    "    \"End station\": \"EndStation Name\",   # Standardize 'End Station Name'\n",
    "    \"Start station number\": \"StartStation Id\", # Map number to Id\n",
    "    \"End station number\": \"EndStation Id\",   # Map number to Id\n",
    "    \"Bike number\": \"Bike Id\",             # Map number to Id\n",
    "    \"Number\": \"Rental Id\"                 # Map 'Number' to 'Rental Id'\n",
    "}\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the specified folder.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files. Loading and Standardizing...\")\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(f\"Attempting to load file {i+1}: {file}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, engine='python') \n",
    "\n",
    "            # --- Standardize column names for the current DataFrame ---\n",
    "            rename_dict_for_this_df = {\n",
    "                variant_name: standard_name\n",
    "                for variant_name, standard_name in column_mapping.items()\n",
    "                if variant_name in df.columns\n",
    "            }\n",
    "\n",
    "            # Apply the renaming if any columns matched the mapping\n",
    "            if rename_dict_for_this_df:\n",
    "                df = df.rename(columns=rename_dict_for_this_df)\n",
    "                print(f\"   Renamed columns in {file}: {list(rename_dict_for_this_df.keys())} -> {list(rename_dict_for_this_df.values())}\")\n",
    "\n",
    "            dataframes.append(df)\n",
    "            i += 1\n",
    "            print(f\"Successfully loaded and processed: {file} with shape {df.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print the error and the file that caused it\n",
    "            print(f\"-----------------------------------------------------\")\n",
    "            print(f\"Error loading file {file}: {e}\")\n",
    "            print(f\"Skipping this file.\")\n",
    "            print(f\"-----------------------------------------------------\")\n",
    "            # continue # Or break if you want to stop on error\n",
    "\n",
    "    # Check if any dataframes were loaded successfully\n",
    "    if dataframes:\n",
    "        print(\"\\nConcatenating DataFrames...\")\n",
    "        combined_df1 = pd.concat(dataframes, axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # Display the combined DataFrame info\n",
    "        print(\"\\nCombined DataFrame Info:\")\n",
    "        print(f\"Shape: {combined_df1.shape}\")\n",
    "\n",
    "        # --- Check NaN percentages again ---\n",
    "        print(\"\\nPercentage of NaN values per column (after standardization):\")\n",
    "        nan_percentage = (combined_df1.isna().sum() / len(combined_df1)) * 100\n",
    "        print(nan_percentage)\n",
    "    else:\n",
    "        print(\"No dataframes were successfully loaded to concatenate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6007afb",
   "metadata": {},
   "source": [
    "#### Standardizing column names in order to merge multiple dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a18678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the mapping from variant names to standard names ---\n",
    "column_mapping = {\n",
    "    \"Start date\": \"Start Date\",           # Standardize 'Start Date'\n",
    "    \"End date\": \"End Date\",             # Standardize 'End Date'\n",
    "    \"Start station\": \"StartStation Name\", # Standardize 'Start Station Name'\n",
    "    \"End station\": \"EndStation Name\",   # Standardize 'End Station Name'\n",
    "    \"Start station number\": \"StartStation Id\", # Map number to Id\n",
    "    \"End station number\": \"EndStation Id\",   # Map number to Id\n",
    "    \"Bike number\": \"Bike Id\",             # Map number to Id\n",
    "    \"Number\": \"Rental Id\"                 # Map 'Number' to 'Rental Id'\n",
    "}\n",
    "\n",
    "folder_path = \"part2\"\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "i = 0\n",
    "print(f\"Looking for CSV files in: {os.path.abspath(folder_path)}\")\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the specified folder.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files. Loading and Standardizing...\")\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(f\"Attempting to load file {i+1}: {file}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, engine='python')\n",
    "\n",
    "            # --- Standardize column names for the current DataFrame ---\n",
    "            rename_dict_for_this_df = {\n",
    "                variant_name: standard_name\n",
    "                for variant_name, standard_name in column_mapping.items()\n",
    "                if variant_name in df.columns\n",
    "            }\n",
    "\n",
    "            # Apply the renaming if any columns matched the mapping\n",
    "            if rename_dict_for_this_df:\n",
    "                df = df.rename(columns=rename_dict_for_this_df)\n",
    "                print(f\"   Renamed columns in {file}: {list(rename_dict_for_this_df.keys())} -> {list(rename_dict_for_this_df.values())}\")\n",
    "\n",
    "            dataframes.append(df)\n",
    "            i += 1\n",
    "            print(f\"Successfully loaded and processed: {file} with shape {df.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print the error and the file that caused it\n",
    "            print(f\"-----------------------------------------------------\")\n",
    "            print(f\"Error loading file {file}: {e}\")\n",
    "            print(f\"Skipping this file.\")\n",
    "            print(f\"-----------------------------------------------------\")\n",
    "            # continue # Or break if you want to stop on error\n",
    "\n",
    "    # Check if any dataframes were loaded successfully\n",
    "    if dataframes:\n",
    "        print(\"\\nConcatenating DataFrames...\")\n",
    "        combined_df2 = pd.concat(dataframes, axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # Display the combined DataFrame info\n",
    "        print(\"\\nCombined DataFrame Info:\")\n",
    "        print(f\"Shape: {combined_df2.shape}\")\n",
    "\n",
    "        # --- Check NaN percentages again ---\n",
    "        print(\"\\nPercentage of NaN values per column (after standardization):\")\n",
    "        nan_percentage = (combined_df2.isna().sum() / len(combined_df2)) * 100\n",
    "        print(nan_percentage)\n",
    "    else:\n",
    "        print(\"No dataframes were successfully loaded to concatenate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([combined_df1, combined_df2], axis=0, ignore_index=True)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e381218",
   "metadata": {},
   "source": [
    "#### Parsing start date and end date since they are in different format in various csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2310f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_df[[\"Bike Id\", \"Start Date\", \"End Date\", \"Rental Id\", \"StartStation Id\", \"StartStation Name\", \"EndStation Id\", \"EndStation Name\"]]\n",
    "\n",
    "# Step 1: Parsing with pandas first (handles most common formats)\n",
    "df['Parsed Start Date'] = pd.to_datetime(df['Start Date'], errors='coerce')\n",
    "\n",
    "# Step 2: Identifying rows that failed to parse\n",
    "mask_failed = df['Parsed Start Date'].isna() & df['Start Date'].notna()\n",
    "\n",
    "# Step 3: Trying dateutil.parser.parse on the failed ones\n",
    "def robust_parse(date_str):\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply only to rows that failed in first attempt\n",
    "df.loc[mask_failed, 'Parsed Start Date'] = df.loc[mask_failed, 'Start Date'].apply(robust_parse)\n",
    "\n",
    "# Optional: Drop or log rows that still couldn't be parsed\n",
    "still_failed = df['Parsed Start Date'].isna() & df['Start Date'].notna()\n",
    "if still_failed.any():\n",
    "    print(f\"Warning: {still_failed.sum()} dates could not be parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0efeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Parsing with pandas first (handles most common formats)\n",
    "df['Parsed End Date'] = pd.to_datetime(df['End Date'], errors='coerce')\n",
    "\n",
    "# Step 2: Identifying rows that failed to parse\n",
    "mask_failed = df['Parsed End Date'].isna() & df['End Date'].notna()\n",
    "\n",
    "# Step 3: Trying dateutil.parser.parse on the failed ones\n",
    "def robust_parse(date_str):\n",
    "    try:\n",
    "        return parser.parse(date_str)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply only to rows that failed in first attempt\n",
    "df.loc[mask_failed, 'Parsed End Date'] = df.loc[mask_failed, 'End Date'].apply(robust_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd99ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate travel duration in minutes\n",
    "df[\"duration_min\"] = (df[\"Parsed End Date\"] - df[\"Parsed Start Date\"]).dt.total_seconds() / 60.0  # Duration in minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Parsed Start Date\"].isnull().sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a16a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf73720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns have NaN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f225a2f4",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d955590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- DataFrame Info ---\")\n",
    "# Display the structure and data types of the dataframe\n",
    "df.info()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8cc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 3 start and end stations based on the number of rentals\n",
    "\n",
    "top_3_start_stations = df['StartStation Name'].value_counts().head(3)\n",
    "top_3_end_stations = df['EndStation Name'].value_counts().head(3)\n",
    "\n",
    "print(\"--- Top Start Stations (up to 3) ---\")\n",
    "print(top_3_start_stations)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Top End Stations (up to 3) ---\")\n",
    "print(top_3_end_stations)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7380d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_within_half_hour = df[df['duration_min'] <= 30].shape[0] \n",
    "\n",
    "# Count the total number of rides\n",
    "total_rides = df.shape[0] \n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_within_half_hour = (rides_within_half_hour / total_rides) * 100\n",
    "percentage_within_half_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfdbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualizing Distribution of Rental duration Data using Plotly ---\n",
    "\n",
    "print(\"--- Generating Plots (using Plotly) ---\")\n",
    "filtered_df = df[(df['duration_min'] > 0) & (df['duration_min'] <= 60)].copy()\n",
    "\n",
    "fig_hist = px.histogram(filtered_df, x='duration_min',\n",
    "                        title='Distribution of Rental Durations',\n",
    "                        labels={'duration_min': 'Duration (minutes)'},\n",
    "                        nbins=6\n",
    "                       )\n",
    "fig_hist.update_layout(bargap=0.1) \n",
    "fig_hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_stations = make_subplots(rows=1, cols=2, subplot_titles=('Top Start Stations', 'Top End Stations'))\n",
    "\n",
    "# Top Start Stations Bar Chart\n",
    "fig_stations.add_trace(\n",
    "    go.Bar(x=top_3_start_stations.index, y=top_3_start_stations.values, name='Start Stations', marker_color='green'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Top End Stations Bar Chart\n",
    "fig_stations.add_trace(\n",
    "    go.Bar(x=top_3_end_stations.index, y=top_3_end_stations.values, name='End Stations', marker_color='orange'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig_stations.update_layout(title_text='Top Start and End Station Analysis', showlegend=False)\n",
    "fig_stations.update_xaxes(title_text=\"Station Name\", row=1, col=1)\n",
    "fig_stations.update_xaxes(title_text=\"Station Name\", row=1, col=2)\n",
    "fig_stations.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig_stations.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
    "fig_stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering & Time-Based Plots\n",
    "df['Start Hour'] = df['Parsed Start Date'].dt.hour\n",
    "df['Start DayOfWeek'] = df['Parsed Start Date'].dt.day_name()\n",
    "\n",
    "# Rentals by Hour of Day\n",
    "hourly_counts = df['Start Hour'].value_counts().sort_index()\n",
    "fig_hour = px.bar(x=hourly_counts.index, y=hourly_counts.values,\n",
    "                  title='Number of Rentals by Hour of Day',\n",
    "                  labels={'x': 'Hour of Day', 'y': 'Number of Rentals'})\n",
    "fig_hour.update_layout(xaxis = dict(tickmode = 'linear', dtick = 1))\n",
    "fig_hour.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rentals by Day of Week\n",
    "daily_counts = df['Start DayOfWeek'].value_counts()\n",
    "\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_counts = daily_counts.reindex(days_order).fillna(0)\n",
    "\n",
    "fig_day = px.bar(x=daily_counts.index, y=daily_counts.values,\n",
    "                 title='Number of Rentals by Day of Week',\n",
    "                 labels={'x': 'Day of Week', 'y': 'Number of Rentals'})\n",
    "fig_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dcb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Performing Monthly and Yearly Distribution Analysis ---\")\n",
    "\n",
    "# 1. Feature Engineering: Extract Month and Year\n",
    "# Check if columns already exist to avoid errors on re-runs\n",
    "if 'Start Month Name' not in df.columns:\n",
    "    df['Start Month Name'] = df['Parsed Start Date'].dt.month_name()\n",
    "if 'Start Year' not in df.columns:\n",
    "    df['Start Year'] = df['Parsed Start Date'].dt.year\n",
    "\n",
    "# 2. Calculate Yearly Distribution\n",
    "yearly_counts = df['Start Year'].value_counts().sort_index()\n",
    "print(\"\\n--- Yearly Rental Counts ---\")\n",
    "print(yearly_counts)\n",
    "\n",
    "# 3. Calculate Monthly Distribution\n",
    "monthly_counts = df['Start Month Name'].value_counts()\n",
    "\n",
    "# Define the correct order for months\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Reindex the monthly counts to ensure chronological order and fill missing months with 0\n",
    "monthly_counts = monthly_counts.reindex(month_order, fill_value=0)\n",
    "\n",
    "print(\"\\n--- Monthly Rental Counts (Ordered) ---\")\n",
    "print(monthly_counts)\n",
    "\n",
    "# 4. Plot Yearly Distribution\n",
    "fig_yearly = px.bar(x=yearly_counts.index, y=yearly_counts.values,\n",
    "                    title='Yearly Distribution of Rentals',\n",
    "                    labels={'x': 'Year', 'y': 'Number of Rentals'},\n",
    "                    text_auto=True) \n",
    "fig_yearly.update_layout(xaxis_type='category')\n",
    "print(\"\\n--- Showing Yearly Distribution Plot ---\")\n",
    "fig_yearly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a07cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot Monthly Distribution\n",
    "fig_monthly = px.bar(x=monthly_counts.index, y=monthly_counts.values,\n",
    "                     title='Monthly Distribution of Rentals',\n",
    "                     labels={'x': 'Month', 'y': 'Number of Rentals'},\n",
    "                     text_auto=True)\n",
    "print(\"\\n--- Showing Monthly Distribution Plot ---\")\n",
    "fig_monthly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f15d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Start Date', 'End Date']\n",
    "df1 = df.drop(columns=columns_to_drop)\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for understanding the underlying usage distribution\n",
    "\n",
    "a = df[df['StartStation Name'] == 'Hyde Park Corner, Hyde Park']\n",
    "routes = a.groupby(['StartStation Name', 'EndStation Name']).size().reset_index(name='Count')\n",
    "# Sorts the routes by count in descending order to find the most frequent\n",
    "most_frequent_routes = routes.sort_values(by='Count', ascending=False).head(5) #to get top 5 routes\n",
    "most_frequent_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dab394",
   "metadata": {},
   "source": [
    "## Data Analytics Usecase 1 - Station Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Station Analysis ---\n",
    "\n",
    "# 1. Identify the most popular start stations\n",
    "#    Counts the occurrences of each unique start station name\n",
    "popular_start_stations = df['StartStation Name'].value_counts().head() \n",
    "print(\"Most Popular Start Stations:\")\n",
    "print(popular_start_stations)\n",
    "print(\"\\n\") \n",
    "\n",
    "# 2. Identify the most popular end stations\n",
    "#    Counts the occurrences of each unique end station name\n",
    "popular_end_stations = df['EndStation Name'].value_counts().head() \n",
    "print(\"Most Popular End Stations:\")\n",
    "print(popular_end_stations)\n",
    "print(\"\\n\") \n",
    "\n",
    "# 3. Identify the most frequently used routes\n",
    "routes = df.groupby(['StartStation Name', 'EndStation Name']).agg(\n",
    "    Count=('Rental Id', 'size'),             # Calculate trip count for the route\n",
    "    MinDuration=('duration_min', 'min'),     # Calculate minimum duration\n",
    "    MedianDuration=('duration_min', 'median'), # Calculate median duration\n",
    "    MaxDuration=('duration_min', 'max')      # Calculate maximum duration\n",
    ").reset_index() # Convert the grouped result back to a DataFrame\n",
    "\n",
    "# Sort the routes by count in descending order to find the most frequent\n",
    "most_frequent_routes = routes.sort_values(by='Count', ascending=False).head(20) \n",
    "most_frequent_routes['MinDuration'] = most_frequent_routes['MinDuration'].clip(lower=0)\n",
    "print(\"Most Frequent Routes with Duration Statistics (Min, Median, Max):\")\n",
    "\n",
    "most_frequent_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d316e0",
   "metadata": {},
   "source": [
    "## Data Analytics Usecase 2 - Bike stock balancing Strategy (Demand-Supply matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bike stock balancing Strategy (Simplified Approach) ---\n",
    "\n",
    "# 1. Calculate Historical Demand (Starts) and Supply (Ends) per Station\n",
    "#    This uses past data as a proxy for current/near-future imbalance.\n",
    "start_counts = df['StartStation Name'].value_counts()\n",
    "end_counts = df['EndStation Name'].value_counts()\n",
    "\n",
    "# 2. Combine into a single DataFrame showing net flow for each station\n",
    "station_flow = pd.DataFrame({\n",
    "    'Demand (Starts)': start_counts,\n",
    "    'Supply (Ends)': end_counts\n",
    "}).fillna(0) # Fill stations that only appear as start or end with 0\n",
    "\n",
    "# Calculate Net Demand: Positive means more bikes leave than arrive (deficit potential)\n",
    "#                        Negative means more bikes arrive than leave (surplus potential)\n",
    "station_flow['Net Demand'] = station_flow['Demand (Starts)'] - station_flow['Supply (Ends)']\n",
    "\n",
    "# 3. Identify Stations with Potential Surplus and Deficit\n",
    "#    Surplus stations have more bikes arriving than leaving (Net Demand < 0)\n",
    "#    Deficit stations have more bikes leaving than arriving (Net Demand > 0)\n",
    "surplus_stations = station_flow[station_flow['Net Demand'] < 0].sort_values('Net Demand')\n",
    "deficit_stations = station_flow[station_flow['Net Demand'] > 0].sort_values('Net Demand', ascending=False)\n",
    "\n",
    "print(\"--- Station Flow Analysis ---\")\n",
    "print(station_flow.head())\n",
    "print(\"\\nPotential Surplus Stations (More bikes arriving):\")\n",
    "print(surplus_stations.head())\n",
    "print(\"\\nPotential Deficit Stations (More bikes leaving):\")\n",
    "print(deficit_stations.head())\n",
    "\n",
    "# 4. Suggest Rebalancing Moves (Basic Pairing)\n",
    "#    This simple example pairs stations with the biggest surplus to those with the biggest deficit.\n",
    "\n",
    "print(\"\\n--- Suggested Rebalancing Moves (Simplified) ---\")\n",
    "# Convert surplus/deficit stations to iterables for pairing\n",
    "surplus_list = list(surplus_stations.index)\n",
    "deficit_list = list(deficit_stations.index)\n",
    "num_moves = min(len(surplus_list), len(deficit_list)) # Max possible pairs\n",
    "\n",
    "if not surplus_list or not deficit_list:\n",
    "    print(\"No rebalancing needed based on historical net demand, or data is insufficient.\")\n",
    "else:\n",
    "    print(\"Move bikes FROM (Surplus) -> TO (Deficit):\")\n",
    "    # Simple pairing: match largest surplus with largest deficit, second largest, etc.\n",
    "    #for i in range(num_moves):\n",
    "    for i in range(5):\n",
    "        from_station = surplus_list[i]\n",
    "        to_station = deficit_list[i]\n",
    "        # Calculate the theoretical number of bikes to move (limited by supply/demand)\n",
    "        bikes_to_move = min(abs(surplus_stations.loc[from_station, 'Net Demand']),\n",
    "                            deficit_stations.loc[to_station, 'Net Demand'])\n",
    "        print(f\"- Move {int(bikes_to_move)} bikes from '{from_station}' to '{to_station}'\")\n",
    "\n",
    "    print(\"\\nNote: This is a simplified suggestion based on overall historical flow.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
